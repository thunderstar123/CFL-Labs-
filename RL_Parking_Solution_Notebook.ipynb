{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined and Optimized RL Parking Allocation Code\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# -------------------------\n",
    "# ENVIRONMENT DEFINITION\n",
    "# -------------------------\n",
    "class ParkingEnv(gym.Env):\n",
    "    def __init__(self, num_spots=10, max_queue=5, peak_hours=[(8, 10), (16, 18)], max_steps=96):\n",
    "        super(ParkingEnv, self).__init__()\n",
    "        self.num_spots = num_spots\n",
    "        self.max_queue = max_queue\n",
    "        self.peak_hours = peak_hours\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'spots': spaces.Box(low=0, high=1, shape=(num_spots,), dtype=np.int32),\n",
    "            'queue': spaces.Discrete(max_queue + 1),\n",
    "            'hour': spaces.Discrete(24)\n",
    "        })\n",
    "        self.action_space = spaces.Discrete(num_spots + 1)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.spots = np.zeros(self.num_spots, dtype=np.int32)\n",
    "        self.durations = np.zeros(self.num_spots, dtype=np.int32)\n",
    "        self.queue = 0\n",
    "        self.time = 8\n",
    "        self.steps = 0\n",
    "        self.total_wait = 0\n",
    "        self.illegal_parking = 0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            'spots': self.spots.copy(),\n",
    "            'queue': self.queue,\n",
    "            'hour': self.time\n",
    "        }\n",
    "\n",
    "    def _is_peak(self):\n",
    "        for start, end in self.peak_hours:\n",
    "            if start <= self.time < end:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _generate_vehicles(self):\n",
    "        rate = 2 if self._is_peak() else 0.5\n",
    "        arrivals = np.random.poisson(rate)\n",
    "        self.queue = min(self.max_queue, self.queue + arrivals)\n",
    "\n",
    "    def _update_spots(self):\n",
    "        for i in range(self.num_spots):\n",
    "            if self.spots[i] == 1:\n",
    "                self.durations[i] -= 15\n",
    "                if self.durations[i] <= 0:\n",
    "                    self.spots[i] = 0\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        reward = 0\n",
    "        if action == self.num_spots:\n",
    "            reward -= 0.2 * self.queue\n",
    "            return reward\n",
    "\n",
    "        if self.spots[action] == 1:\n",
    "            reward -= 5\n",
    "            self.illegal_parking += 1\n",
    "            return reward\n",
    "\n",
    "        if self.queue > 0:\n",
    "            reward += 2\n",
    "            center_dist = abs(action - self.num_spots // 2) / (self.num_spots // 2)\n",
    "            reward += (1 - center_dist)\n",
    "            self.spots[action] = 1\n",
    "            self.durations[action] = np.random.randint(30, 121)\n",
    "            self.queue -= 1\n",
    "        else:\n",
    "            reward -= 1\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = self._calculate_reward(action)\n",
    "        self._update_spots()\n",
    "        self._generate_vehicles()\n",
    "        self.time = int((self.time + 0.25) % 24)\n",
    "        self.total_wait += self.queue * 15\n",
    "        self.steps += 1\n",
    "        done = self.steps >= self.max_steps\n",
    "        info = {\n",
    "            'avg_wait': self.total_wait / (self.steps + 1e-6),\n",
    "            'illegal': self.illegal_parking,\n",
    "            'utilization': np.mean(self.spots)\n",
    "        }\n",
    "        return self._get_obs(), reward, done, False, info\n",
    "\n",
    "# -------------------------\n",
    "# DQN NETWORK AND AGENT\n",
    "# -------------------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_spots):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_spots + 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_spots + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = torch.cat([\n",
    "            torch.FloatTensor(obs['spots']),\n",
    "            torch.FloatTensor([obs['queue']]),\n",
    "            torch.FloatTensor([obs['hour'] / 24.0])\n",
    "        ])\n",
    "        return self.fc(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.policy = DQN(env.num_spots)\n",
    "        self.target = DQN(env.num_spots)\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-4)\n",
    "        self.epsilon = 1.0\n",
    "        self.eps_end = 0.1\n",
    "        self.gamma = 0.99\n",
    "        self.batch = 64\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        with torch.no_grad():\n",
    "            q = self.policy(state)\n",
    "            return torch.argmax(q).item()\n",
    "\n",
    "    def remember(self, s, a, r, s2, done):\n",
    "        self.memory.append((s, a, r, s2, done))\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch)\n",
    "        for s, a, r, s2, d in batch:\n",
    "            q_vals = self.policy(s)\n",
    "            with torch.no_grad():\n",
    "                next_q = self.target(s2)\n",
    "                max_q = torch.max(next_q)\n",
    "                target = r + self.gamma * max_q * (1 - d)\n",
    "            q_vals[a] = target\n",
    "            loss = nn.MSELoss()(self.policy(s), q_vals)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def train(self, episodes=500):\n",
    "        for ep in range(episodes):\n",
    "            s, _ = self.env.reset()\n",
    "            done = False\n",
    "            total = 0\n",
    "            while not done:\n",
    "                a = self.act(s)\n",
    "                s2, r, done, _, info = self.env.step(a)\n",
    "                self.remember(s, a, r, s2, done)\n",
    "                self.learn()\n",
    "                s = s2\n",
    "                total += r\n",
    "            if ep % 10 == 0:\n",
    "                self.target.load_state_dict(self.policy.state_dict())\n",
    "            self.epsilon = max(self.eps_end, self.epsilon * 0.995)\n",
    "            print(f\"Ep {ep}, Reward: {total:.2f}, Eps: {self.epsilon:.2f}, Illegal: {info['illegal']}, Wait: {info['avg_wait']:.2f}, Util: {info['utilization']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
